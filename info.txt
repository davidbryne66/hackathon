Hackathon Use Case: Adventure Works AI
Event: Team Day Hackathon Duration: 3 Hours Theme: Integrating Generative AI into Data & Analytics Solutions
1. Introduction & Goal
Welcome to the Adventure Works AI Hackathon!
Today's challenge is designed to be a hands-on exploration of how generative AI can revolutionize our workflow as a data and analytics team. Over the next three hours, your team will build an end-to-end analytics solution on Google Cloud, leveraging GenAI tools at every step to accelerate development, enhance creativity, and improve the final product.
The goal is not just to complete the tasks, but to actively discover and document how tools like Google's Gemini, Cursor, or other generative AI assistants can be integrated into our daily processesâ€”from data modeling to final user interaction.
2. The Scenario
You are a data & analytics team at Adventure Works Cycles, a global manufacturer and seller of bicycles and accessories. The company's transactional data is currently stored in a traditional OLTP (Online Transaction Processing) database. The business wants to unlock deeper insights into sales performance, product popularity, and customer behavior.
Your mission is to build a modern analytics stack on Google Cloud that enables powerful, intuitive data exploration for business stakeholders.
3. The Challenge: End-to-End Analytics Pipeline
You will progress through five distinct phases, starting with raw source data and ending with a conversational analytics interface.
Phase 1: Dimensional Data Modeling (Estimated Time: 45 mins)
Task: Design a dimensional model (star schema) suitable for analytics based on the Microsoft Adventure Works OLTP database schema. Your model should be optimized for business intelligence queries, focusing on sales and product performance.
Your Deliverable:
A diagram or document outlining your star schema, including fact tables (e.g., fct_sales) and dimension tables (e.g., dim_product, dim_customer, dim_date, dim_territory).
The DDL (Data Definition Language) CREATE TABLE statements for your new BigQuery tables.
ðŸ’¡ Generative AI Integration Ideas:
Schema Understanding: Provide the source Adventure Works schema DDL to a tool like Gemini and ask it to explain the relationships between key tables (e.g., SalesOrderHeader, SalesOrderDetail, Product, Customer).
Brainstorming Dimensions & Facts: Ask your AI assistant: "Given this OLTP schema for a bicycle manufacturer, propose a star schema to analyze sales performance. What would be the central fact table? What dimensions would be most useful?"
DDL Generation: Once you've settled on a model, describe your desired tables to the AI (e.g., "Generate a BigQuery DDL script for a dimension table called dim_product with columns for product key, name, category, subcategory, and color") and have it write the SQL for you.
Phase 2: Source-to-Target Mapping (Estimated Time: 45 mins)
Task: Create a source-to-target mapping document. This document specifies which source tables and columns from the Adventure Works database will populate each column in your new BigQuery dimensional model. It should also describe any transformations required (e.g., joins, calculations, data type conversions, business logic).
Your Deliverable:
A spreadsheet or markdown table detailing the mapping for at least one fact table and two dimension tables.
ðŸ’¡ Generative AI Integration Ideas:
Automated Mapping: Paste the DDL for both a source table (e.g., Production.Product) and your target table (e.g., dim_product) and ask the AI to "Create a source-to-target mapping between these two tables, identifying direct mappings and suggesting transformations for any derived fields."
Transformation Logic: For a complex field like order_total in your fct_sales table, ask the AI: "Write the business logic in pseudocode to calculate the total order amount by joining SalesOrderHeader and SalesOrderDetail from the source schema."
Phase 3: Data Transformation with Dataform (Estimated Time: 60 mins)
Task: Using Google Cloud Dataform, implement the ETL (Extract, Transform, Load) logic defined in your mapping document. Write the SQLX scripts to populate your new dimensional model tables in BigQuery from the raw Adventure Works data.
Your Deliverable:
A functioning Dataform project with SQLX files that successfully create and populate your fact and dimension tables.
ðŸ’¡ Generative AI Integration Ideas:
Code Generation: Use an AI-powered IDE like Cursor or the Gemini in Cloud Code extension. Feed it your transformation logic from Phase 2 (e.g., "Write a Dataform SQLX query to create the dim_customer table by joining Sales.Customer and Person.Person") and let it generate the base query.
Debugging: If a query fails, paste the SQL code and the error message into your AI tool and ask it to "Debug this BigQuery SQL error."
Code Documentation & Quality: Ask the AI to "Add comments to this SQLX file explaining the logic" or "Refactor this query to make it more readable and performant."
Phase 4: Looker LookML Semantic Layer (Estimated Time: 30 mins)
Task: Create a simple Looker LookML model based on the new tables in BigQuery. This semantic layer will define dimensions, measures, and joins, making it easy for business users to self-serve.
Your Deliverable:
A LookML project with at least one model, one explore, and view files for your fact and dimension tables. The explore should allow users to analyze sales amount by product category and customer location.
ðŸ’¡ Generative AI Integration Ideas:
Boilerplate Generation: Provide your BigQuery table DDL to the AI and ask it to "Generate a LookML view file for this table. Identify potential dimensions and measures."
Explore Creation: Describe the relationships in your model: "Generate a LookML explore that joins the fct_sales fact table with dim_product, dim_customer, and dim_date."
Phase 5: Conversational Analytics Interface (Estimated Time: 30 mins)
Task: Implement a proof-of-concept conversational interface. The goal is to allow a business user to ask a question in plain English (e.g., "What were our total sales for road bikes in North America last year?") and get an answer from your data model.
Your Deliverable:
A simple web app, Colab notebook, or other interface that takes a natural language query as input, uses an LLM to process it, and queries the Looker model (via its API) to retrieve the data.
ðŸ’¡ Generative AI Integration Ideas:
Natural Language to API: This is the core of the task. Use a GenAI model (like Gemini) as the "brain." Your prompt should instruct the model: "You are a data analyst assistant. Your goal is to translate a user's question into a JSON query for the Looker API. Here is the documentation for the LookML model and the API..."
Frontend Code: Use an AI tool to quickly generate the boilerplate HTML/JavaScript/Python code for a simple user interface.
Evaluation Criteria
Teams will be evaluated based on:
Innovative Use of GenAI: How effectively and creatively did you use AI tools in each phase?
Solution Completeness: How far did you get through the five phases?
Technical Quality: The soundness of your data model, the efficiency of your Dataform code, and the usability of your final interface.
Team Collaboration & Presentation: How well you explain your solution and your GenAI integration learnings.

